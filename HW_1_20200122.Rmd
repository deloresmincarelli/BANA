---
output:
  word_document: default
  pdf_document: default
  html_document: default
---
--------------------------
Homework 1 -- Group 8
-------------------------

```{r setup, include=FALSE}
#setwd("H://_UC/7046DataMining1/HW1")
getwd()
```

# Classification and Clustering of the Iris dataset 
Classification and clustering are two important topics in Data Mining.  In this paper, we explore physical characteristics of 3 Iris species and create two models.  (1) The classification model uses what we know about each species to create a model.  When new data is introduced into the model, it will try to match that data to one of the known species. (2) Alternatively, the clustering model does not assume any target goal; rather, we put everything we know for all of our observations into the clustering algorithm and it determines how to group like observations together.  
```{r include = FALSE}

##################################################################################
# Import data and libraries
##################################################################################
pkgs <- c(
  "animation",   # for pre-built statistical animations
  "dplyr",       # for data wrangling
  "ggplot2",     # for drawing nicer graphics
  "gridExtra",   # for grid.arrange() function
  "HistData",    # for historical data sets
  "investr",     # for inverse estimation
  "magick",      # for working with images
  "roundhouse" ,  # for pure awesomeness
  "GGally",  # for gggplot2 extensions
  "pdp",     # for (corrected) Boston housing data
  "plotly",  # for interactive plots
  "tibble",  # for nicer data frames
  "vip" ,     # for variable importance plots
  "class", 
  "gmodels",   #used to make table of results of model
  "FNN", 
  "factoextra"
)

# Install required (CRAN) packages
for (pkg in pkgs) {
  if (!(pkg %in% installed.packages()[, "Package"])) {
    install.packages(pkg)
  }
}


library(datasets)
library(FNN)
library(caret)
library(dplyr)
library(e1071)
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering visualization
library(dendextend) # for comparing two dendrograms
library(ggplot2)
library(magrittr)
library(class)
```
#### Summary of Iris data
```{r echo = FALSE}

#help(iris)
#summary(iris)
#head(iris)
#str(iris)
```
Number of observations for each species
```{r echo = FALSE}
# Division of `Species`
table(iris$Species)
```
Percent of each species
```{r echo = FALSE}
# Percentual division of `Species`
round(prop.table(table(iris$Species)) * 100, digits = 1)
```


```{r echo = FALSE}
#class(iris)
#summary(iris[c("Petal.Width", "Sepal.Width")])
```
#### Statistics
As we look at the median, min and max of each variable we see that they all fall in a similar range. We may still need to normalize the data so that each variable has "equal footing".  
```{r echo = FALSE}
irisdata.nums<-iris[,1:4]
sapply(irisdata.nums, function(irisdata.nums) c( "Stand dev" = sd(irisdata.nums), 
                         "Mean"= mean(irisdata.nums,na.rm=TRUE),
                         "n" = length(irisdata.nums),
                         "Median" = median(irisdata.nums),
                         "CoeffofVariation" = sd(irisdata.nums)/mean(irisdata.nums,na.rm=TRUE),
                         "Minimum" = min(irisdata.nums),
                         "Maximun" = max(irisdata.nums),
                         "1st Quantile" = quantile(irisdata.nums,.25),
                         "3rd Quartile" = quantile(irisdata.nums,.75)
                    )
)
```

#### Lets explore some relationships
First, we visualize a relationship between sepal length and width for each species.   The Setosa species has a shorter length and greater width than the othes. Also, Virginica has a larger range sepal length than the others.  This is an encouraging sign that there are distinguishing characteristics that our models can work with.  
```{r echo = FALSE}
# loading packages
# library(ggplot2)
# library(magrittr)
# library(class)

# sepal width vs. sepal length
SepalScatter<-iris %>%
  ggplot(aes(x=Sepal.Length, y=Sepal.Width, color=Species)) +
  geom_point()
print(SepalScatter + labs(title= "Sepal Width & Length by Species",y="Sepal Width (cm)", x = "Sepal Length (cm)"))


```
```{r echo = FALSE}

```
Next, for Petal width and length, Setosa stands alone with a smaller length and width. Virginica shows a little more variability in width and length, and we can see a few observations of Versicolor creeping into the Virginica territory.  Overall, we see patterns and this will lend itself well toward classification and clustering. 

```{r echo = FALSE}
# Petal
PetalScatter<-iris %>%
  ggplot(aes(x=Petal.Length, y=Petal.Width, color=Species)) +
  geom_point()
print(PetalScatter + labs(title= "Petal Width & Length by Species",y="Petal Width (cm)", x = "Petal Length (cm)"))


```
```{r echo = FALSE}
# I dont think any of this stuff adds value
# Overall correlation `Petal.Length` and `Petal.Width`
#cor(iris$Petal.Length, iris$Petal.Width)
# Return values of `iris` levels 
# x=levels(iris$Species)
# 
# # Print Setosa correlation matrix
# print(x[1])
# cor(iris[iris$Species==x[1],1:4])
# 
# # Print Versicolor correlation matrix
# print(x[2])
# cor(iris[iris$Species==x[2],1:4])
# 
# # Print Virginica correlation matrix
# print(x[3])
# cor(iris[iris$Species==x[3],1:4])

# Libraries
```

#### Density Plots
These plots show us how the data is distributed.  Unlike a histogram, density plots are not subject to the number of bins selected (which can radically change how the distribution appears.)

Interpretation: The "flattness" of then Sepal Length density indicates many observations in the 5 to 6.5 cm range.  

```{r echo = FALSE}



# Make the histogram
SL_Density<-iris %>%
  ggplot( aes(x=Sepal.Length)) +
    geom_density(fill="gray", color="black", alpha=0.8) +
    ggtitle("Sepal Length")
print(SL_Density + labs(title= "Sepal Length",y="Density", x = "Sepal Length (cm)"))

```

On the other hand, Sepal width has a lot of observations in a vary narrow range, represented by the narrow spike around 3cm.

```{r echo = FALSE}
SW_Density<-iris %>%
  ggplot( aes(x=Sepal.Width)) +
    geom_density(fill="gray", color="black", alpha=0.8) +
    ggtitle("Sepal Width")
print(SW_Density + labs(title= "Sepal Width",y="Density", x = "Sepal Width (cm)"))


```

The petal density plots show a bimodal pattern, most likely due to the sentosa species.  Recall, in the scatter plots, sentosa was grouped far from the other two species.  

```{r echo = FALSE}
PL_Density<-iris %>%
  ggplot( aes(x=Petal.Length)) +
    geom_density(fill="gray", color="black", alpha=0.8) +
    ggtitle("Petal Length")
print(PL_Density + labs(title= "Petal Length",y="Density", x = "Petal Length (cm)"))
```

```{r echo = FALSE}
PW_Density<-iris %>%
  ggplot( aes(x=Petal.Width)) +
    geom_density(fill="gray", color="black", alpha=0.8) +
    ggtitle("Petal Width")
print(PW_Density + labs(title= "Petal Width",y="Density", x = "Petal Width (cm)"))

```

## Classification 
### Problem:  
Given a dataset with physical characteristics of iris flowers and their known species, how can we accurately categorize new observations into the correct species? 

### Approach
We will use the K-Nearest Neighbor (KNN) algorithm to classify the new flowers.  The KNN algorithm uses a "similarity" measure to assess which "known" species the new data is most similar to.  Basically, a new datapoint is compared to it's proximity to it's neighbor datapoints.  We can specify the number of neighbors to use, and will start with k=3.  (It is good to use odd  numbers to break a tie). 

We will separate our data into a training (80%) and testing set (20%).  Using the training data, we will train our model to recognize characteristics of setosa, virginica and versicolor species and then assess performance using a testing dataset.    

### Performance
We can see that when k = 3, our accuracy is 93.3% (.9333).  There were two points that were mis-classified as versicolor instead of viginica.  
As we try other values of k, we see that we can achieve up to 96.67%  (k = 1).  

```{r echo = FALSE}
# Partitioning
# Here we partition the data into training (80%) and validation (20%) sets.
# partition the data
set.seed(7046)
train.index <- sample(nrow(iris), nrow(iris) * 0.8)
valid.index <- as.numeric(setdiff(rownames(iris), train.index))
iris.train <- iris[train.index, ]
iris.valid <- iris[valid.index, ]

# remember to exclude both outcome variables from your list of predictors
nn <- knn(train = iris.train[, 1:4], test = iris.valid[, 1:4], cl = iris.train$Species, k = 3)

confusionMatrix(nn, as.factor(iris.valid$Species), positive = "1")
```
Trying various levels of k.  

```{r echo = FALSE}
#Determine Best k
#We will now determine the optimal value for k by evaluating the accuracy at several levels k. 
#We'll look at values of k 1-20, which is generally sufficient.

set.seed(7046)

# initialize a data frame with two columns: k and accuracy, note rep(0,20) provides a list of 20 zeros
accuracy.df <- data.frame(k = seq(1, 10, 1), accuracy = rep(0, 10))

# compute knn for different k on validation set
for (i in 1:10) {
  knn.pred <- knn(train = iris.train[, 1:4], test = iris.valid[, 1:4], 
                  cl = iris.train$Species, k = i)
  accuracy.df[i, 2] <- confusionMatrix(knn.pred, as.factor(iris.valid$Species), positive = "1")$overall[1]
}
accuracy.df

```

```{r}

```
## Clustering using K-Means
### Problem
Clustering involves grouping objects together that are "most like" one another. Objects in one cluster will be different from objects in another cluster, and one observation can belong to only one cluster. There is no targeted "answer" or "truth" that you can objectively measure your result against, because the purpose of clustering is to mathematically assess similarities and provide insight to us that we may never otherwise observe.  

Given the iris dataset, how will K-Means organize the data into meaningful groups?  Will the result look like our species groupings that we already know?

### Approach
With K-Means, the only direction we provide is the number of clusters we desire as output.  We can use our domain knowledge and start with k=3.   There are also functions that we can use to determine the "optimal" number of clusters.  Ultimately, the best number of clusters is what works for the business purpose.

We also will require the algorithm to try different values of the starting cluster assignment, (assigned to each observation), which is the first step in it's computation.  This random value can influence the result, so we specify the function to try 20 variations and choose the result with the minimum within-cluster variation.  

Should we use all of the variables?  Recall from the scatter plots, the Sepal width and length did not show a lot of separation for versicolor and virginica.  Therefore, we will first try all 4 variables and then we'll just use Petal width and length.  

### First, we'll use all 4 variables, and specify that we want 3 clusters.  
```{r echo = FALSE}
# Used reference at https://www.r-bloggers.com/k-means-clustering-in-r/


set.seed(7046)
irisKMCluster <- kmeans(iris[, 1:4], 3, nstart = 20)
irisKMCluster
#K-means clustering with 3 clusters of sizes 38, 62, 50



```
#### Now, we'll use just Petal Length and Width, keeping k = 3 clusters.     
```{r echo = FALSE}
set.seed(7046)
irisKMCluster_Ptl <- kmeans(iris[, 3:4], 3, nstart = 20)
irisKMCluster_Ptl

```
### Performance:  
Compared to our known subject-matter knowledge on each observation's species, the K - Means perfectly clusters all of the Setosa flowers together.  However, when we used all 4 variables,  K-Means had trouble with Virginica, mis-categorizing 14 observationis, and 2 mis-categorized for Versicolor.   Net, 89.33% were clustered within groups of their own species. 

Suprisingly, when we limited the variables to just Petal Length and Petal Width, we achieved clusters that look closer to what our business user want, with 96% of our data grouped into the correct species.   
```{r echo = FALSE}
print("Using all 4 variables")
table(irisKMCluster$cluster, iris$Species)

print("Using just Petal Length and Petal Width")
table(irisKMCluster_Ptl$cluster, iris$Species)
   
```

## Hierarchical Clustering
### Problem
In this scenario will will try a different clustering algorith to compare to K - Means.  Hierarchical clustering does not require us to pre-specify the number of clusters to be generated as is required by the k-means approach. Furthermore, hierarchical clustering results in a tree representation of the observations, called a dendrogram.  However, it is computationally expensive, so this approach may not work with large datasets.  

### Approach
Data preparation:  We standardized the data  (i.e., scaled) to make variables comparable. Standardization consists of transforming the variables such that they have mean zero and standard deviation one. 

We used "Agglomerative clustering" ( AGNES - Agglomerative Nesting). It works in a bottom-up manner. That is, each object is initially considered as a single-element cluster (leaf). At each step of the algorithm, the two clusters that are the most similar are combined into a new bigger cluster (nodes). This procedure is iterated until all points are member of just one single big cluster (root) (see figure below). The result is a tree which can be plotted as a dendrogram.

### Scenario 1 :  Use all 4 variables.  
(We will also use the "complete" linkage method - Maximal inter-cluster dissimilarity)
We will cut the tree to 3 clusters.  118/150 = 78.67% fell within clusters of their known species.  
```{r echo = FALSE}
#Reference https://uc-r.github.io/hc_clustering
# To perform a cluster analysis in R, generally, the data should be prepared as follows:
# 
# Rows are observations (individuals) and columns are variables
# Any missing value in the data must be removed or estimated.
# The data must be standardized (i.e., scaled) to make variables comparable. Recall that, standardization consists of transforming the variables such that they have mean zero and standard deviation one.[^scale]
set.seed(7046)
iris_sc <- scale(iris[,1:4])

irisspecies <- as.list(iris$Species)
irisspecies <- unlist(irisspecies)

# Agglomerative clustering: It’s also known as AGNES (Agglomerative Nesting). It works in a bottom-up manner. That is, each object is initially considered as a single-element cluster (leaf). At each step of the algorithm, the two clusters that are the most similar are combined into a new bigger cluster (nodes). This procedure is iterated until all points are member of just one single big cluster (root) (see figure below). The result is a tree which can be plotted as a dendrogram.
# 
# We can perform agglomerative HC with hclust. First we compute the dissimilarity values with dist and then feed these values into hclust and specify the agglomeration method to be used (i.e. “complete”, “average”, “single”, “ward.D”). We can then plot the dendrogram.

# Dissimilarity matrix
distance <- dist(iris_sc, method = "euclidean")

# Hierarchical clustering using Complete Linkage
hc1 <- hclust(distance, method = "complete" )

# Plot the obtained dendrogram
#plot(hc1, cex = 0.6, hang = -1)

#run these two lines of code together or it wont work
plot(hc1, cex = 0.6)
rect.hclust(hc1, k = 3, border = 2:5)

hcl_cut <- cutree(hc1, k = 3)
table(irisspecies, hcl_cut)
```


```{r echo = FALSE}

```
```{r echo = FALSE}
```

### Scenario 2 :  Use only Petal Length and Width variables.  
(We will also use the "complete" linkage method - Maximal inter-cluster dissimilarity)
We will cut the tree to 3 clusters.  125/150 = 83.3% fell within clusters of their known species. 
```{r echo = FALSE}

set.seed(7046)
iris_sc_2 <- scale(iris[,3:4])

distance <- dist(iris_sc_2, method = "euclidean")

hc2 <- hclust(distance, method = "complete" )

# Plot the obtained dendrogram
plot(hc2, cex = 0.6, hang = -1)

# Cut tree into 3 groups
hc2_cut <- cutree(hc2, k = 3)

table(irisspecies, hc2_cut)
```

  

```{r echo = FALSE}
#Optimal Number of Clusters
#https://www.rdocumentation.org/packages/factoextra/versions/1.0.6/topics/fviz_nbclust
#set.seed(7046)
#fviz_nbclust(iris_sc, hcut, method = "wss")
```
### Performance
Similar to K-Means, we find that we get better performance when we limit the variables to just Petal Length and Width -- 83.3% compared to 78.67%.  
Here we can visualize the 3 clusters in a two dimensional space using just Petal Length and Width.  You can see overlap between the green and blue planes which is where we are seeing the most mis-classifications.   
```{r echo = FALSE}
fviz_cluster(list(data = iris[,1:4], cluster = hc2_cut))
```

# Conclusions  -- need to expand on this

1. Sometimes more data isn't always better.  (Ex: using all data vs just some of the variables)
2. K means vs hierichal  
etc. 

```{r echo = FALSE}


```

